{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10755766,"sourceType":"datasetVersion","datasetId":6671186},{"sourceId":10755779,"sourceType":"datasetVersion","datasetId":6671196},{"sourceId":256586,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":204046,"modelId":225262},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902},{"sourceId":256571,"sourceType":"modelInstanceVersion","modelInstanceId":204044,"modelId":225262}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings  # Using Hugging Face embeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\n\n# ---------------------------\n# 1. PDF Processing\n# ---------------------------\n\ndef process_pdf(pdf_path, chunk_size=1000, chunk_overlap=100):\n    \"\"\"\n    Loads a PDF file, extracts its text, and splits the text into chunks.\n    \"\"\"\n    loader = PyPDFLoader(pdf_path)\n    documents = loader.load()\n    \n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    docs = text_splitter.split_documents(documents)\n    \n    return docs\n\n# ---------------------------\n# 2. JSON Processing\n# ---------------------------\n\ndef flatten_json(y):\n    \"\"\"\n    Flattens a nested JSON/dictionary.\n    \"\"\"\n    out = {}\n    \n    def flatten(x, name=''):\n        if isinstance(x, dict):\n            for a in x:\n                flatten(x[a], name + a + '_')\n        elif isinstance(x, list):\n            for i, a in enumerate(x):\n                flatten(a, name + str(i) + '_')\n        else:\n            out[name[:-1]] = x  # remove the trailing underscore\n            \n    flatten(y)\n    return out\n\ndef process_json(json_path):\n    \"\"\"\n    Loads a JSON file, extracts and normalizes text fields.\n    \"\"\"\n    with open(json_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    texts = []\n    if isinstance(data, list):\n        for entry in data:\n            flat_entry = flatten_json(entry)\n            combined_text = \" \".join(str(v) for v in flat_entry.values() if isinstance(v, str))\n            texts.append(combined_text)\n    elif isinstance(data, dict):\n        flat_entry = flatten_json(data)\n        combined_text = \" \".join(str(v) for v in flat_entry.values() if isinstance(v, str))\n        texts.append(combined_text)\n    \n    # Convert texts into Document objects (for consistency with LangChain)\n    documents = [Document(page_content=text) for text in texts]\n    return documents\n\n# ---------------------------\n# 3. Build a Vector Store for Efficient Retrieval Using Hugging Face Embeddings\n# ---------------------------\n\ndef build_vector_store(documents):\n    \"\"\"\n    Converts documents into embeddings using a Hugging Face model and stores them in a FAISS vector store.\n    \"\"\"\n    # Initialize the embedding model using a SentenceTransformer model (e.g., 'all-MiniLM-L6-v2')\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    \n    # Build the FAISS index from the documents\n    vectorstore = FAISS.from_documents(documents, embeddings)\n    return vectorstore\n\n# ---------------------------\n# 4. Example Usage\n# ---------------------------\n\nif __name__ == '__main__':\n    # Process PDF file\n    pdf_file_path = '/kaggle/input/iba-dataset-pdf/pa-2024-25.pdf'  # Replace with your PDF file path\n    pdf_docs = process_pdf(pdf_file_path)\n    \n    # Process JSON file\n    json_file_path = '/kaggle/input/iba-dataset-json/courses_info.json'  # Replace with your JSON file path\n    json_docs = process_json(json_file_path)\n    \n    # Combine documents from both sources\n    all_documents = pdf_docs + json_docs\n    \n    # Build the vector store for efficient retrieval\n    vector_store = build_vector_store(all_documents)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"faiss_index\"\nvector_store.save_local(save_path)\nprint(f\"Vector store saved to '{save_path}'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nFAISS Index Details:\")\n# Print the index object\nprint(vector_store.index)\n# Print the total number of vectors in the index\nprint(\"Number of vectors in the index:\", vector_store.index.ntotal)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings  # Using Hugging Face embeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore.document import Document\n\n# ---------------------------\n# 1. PDF Processing\n# ---------------------------\ndef process_pdf(pdf_path, chunk_size=1000, chunk_overlap=100):\n    \"\"\"\n    Loads a PDF file, extracts its text, and splits the text into chunks.\n    \"\"\"\n    loader = PyPDFLoader(pdf_path)\n    documents = loader.load()\n    \n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    docs = text_splitter.split_documents(documents)\n    \n    return docs\n\n# ---------------------------\n# 2. JSON Processing\n# ---------------------------\ndef flatten_json(y):\n    \"\"\"\n    Flattens a nested JSON/dictionary.\n    \"\"\"\n    out = {}\n    \n    def flatten(x, name=''):\n        if isinstance(x, dict):\n            for a in x:\n                flatten(x[a], name + a + '_')\n        elif isinstance(x, list):\n            for i, a in enumerate(x):\n                flatten(a, name + str(i) + '_')\n        else:\n            out[name[:-1]] = x  # remove the trailing underscore\n            \n    flatten(y)\n    return out\n\ndef process_json(json_path):\n    \"\"\"\n    Loads a JSON file, extracts and normalizes text fields.\n    \"\"\"\n    with open(json_path, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    texts = []\n    if isinstance(data, list):\n        for entry in data:\n            flat_entry = flatten_json(entry)\n            combined_text = \" \".join(str(v) for v in flat_entry.values() if isinstance(v, str))\n            texts.append(combined_text)\n    elif isinstance(data, dict):\n        flat_entry = flatten_json(data)\n        combined_text = \" \".join(str(v) for v in flat_entry.values() if isinstance(v, str))\n        texts.append(combined_text)\n    \n    # Convert texts into Document objects (for consistency with LangChain)\n    documents = [Document(page_content=text) for text in texts]\n    return documents\n\n# ---------------------------\n# 3. Build a Vector Store for Efficient Retrieval Using Hugging Face Embeddings\n# ---------------------------\ndef build_vector_store(documents):\n    \"\"\"\n    Converts documents into embeddings using a Hugging Face model and stores them in a FAISS vector store.\n    \"\"\"\n    # Initialize the embedding model using a SentenceTransformer model (e.g., 'all-MiniLM-L6-v2')\n    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n    \n    # Build the FAISS index from the documents\n    vectorstore = FAISS.from_documents(documents, embeddings)\n    return vectorstore\n\n# ---------------------------\n# 4. Main Execution: Process, Build, Save, and Print FAISS Index Data\n# ---------------------------\nif __name__ == '__main__':\n    # Process PDF file\n    pdf_file_path = '/kaggle/input/iba-dataset-pdf/pa-2024-25.pdf'  # Replace with your PDF file path\n    pdf_docs = process_pdf(pdf_file_path)\n    \n    # Process JSON file\n    json_file_path = '/kaggle/input/iba-dataset-json/courses_info.json'  # Replace with your JSON file path\n    json_docs = process_json(json_file_path)\n    \n    # Combine documents from both sources\n    all_documents = pdf_docs + json_docs\n    \n    # Build the vector store for efficient retrieval\n    vector_store = build_vector_store(all_documents)\n    \n    # Save the vector store locally (for example, in a directory named 'faiss_index')\n    save_path = \"faiss_index\"\n    vector_store.save_local(save_path)\n    print(f\"Vector store saved to '{save_path}'.\")\n    \n    # ---------------------------\n    # Print FAISS Index Details\n    # ---------------------------\n    print(\"\\nFAISS Index Details:\")\n    # Print the FAISS index object\n    print(vector_store.index)\n    # Print the total number of vectors in the index\n    print(\"Number of vectors in the index:\", vector_store.index.ntotal)\n    \n    # ---------------------------\n    # Print the Data Stored in the FAISS Index\n    # ---------------------------\n    print(\"\\nDocuments stored in the FAISS index:\")\n    # The documents are stored in the docstore (as a dictionary)\n    for doc_id, doc in vector_store.docstore._dict.items():\n        print(f\"Document ID: {doc_id}\")\n        print(f\"Content: {doc.page_content}\")\n        print(\"=\" * 80)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\n\n# Path to the FAISS index file saved by LangChain\nindex_path = \"faiss_index/index.faiss\"\n\n# Load the raw FAISS index\nfaiss_index = faiss.read_index(index_path)\nprint(\"Loaded FAISS index using faiss.read_index:\")\nprint(faiss_index)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Initialize the same embeddings model used when saving the index\nembeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Load the FAISS vector store from the saved directory\nvector_store = FAISS.load_local(\"/kaggle/working/faiss_index\", embeddings, allow_dangerous_deserialization=True)\nprint(\"Loaded vector store using FAISS.load_local:\")\nprint(\"FAISS index object:\", vector_store.index)\nprint(\"Number of vectors in the index:\", vector_store.index.ntotal)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Initialize the same embeddings model used when saving the index\nembeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Load the FAISS vector store from the saved directory\nvector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n\n# Print basic details about the FAISS index\nprint(\"FAISS index details:\")\nprint(vector_store.index)\nprint(\"Number of vectors in the index:\", vector_store.index.ntotal)\n\n# Print the stored documents\nprint(\"\\nDocuments stored in the FAISS vector store:\")\nfor doc_id, doc in vector_store.docstore._dict.items():\n    print(f\"\\nDocument ID: {doc_id}\")\n    print(f\"Content: {doc.page_content}\")\n    print(\"=\" * 80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Define the model name and initialize the embeddings model\nmodel_name = \"all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=model_name)\n\n# Load the FAISS vector store from the saved directory\nvector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n\n# Print context information about the embedding model\nprint(\"Embedding Model Context:\")\nprint(f\"Model Name: {model_name}\")\nprint(\"Description: 'all-MiniLM-L6-v2' is a SentenceTransformer model that provides efficient and effective text embeddings optimized for semantic similarity tasks.\")\nprint(\"=\" * 80)\n\n# Define your query\nquery = \"List down the courses tought by Dr. Anum Tariq\"\n\n# Run a similarity search to retrieve the top 3 matching documents\nretrieved_docs = vector_store.similarity_search(query, k=3)\n\n# Print the retrieved documents along with context\nprint(\"\\nRetrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs):\n    print(f\"\\nDocument {i+1}:\\n{doc.page_content}\\n{'-' * 80}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain import PromptTemplate, LLMChain\nimport transformers\n\n# ---------------------------\n# 1. Load the FAISS Vector Store\n# ---------------------------\nmodel_name = \"all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=model_name)\nvector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\nprint(f\"FAISS index loaded successfully. Number of vectors in the index: {vector_store.index.ntotal}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T12:52:12.973030Z","iopub.execute_input":"2025-02-15T12:52:12.973350Z","iopub.status.idle":"2025-02-15T12:52:22.390960Z","shell.execute_reply.started":"2025-02-15T12:52:12.973324Z","shell.execute_reply":"2025-02-15T12:52:22.390189Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-1-438540e4f54c>:11: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n","output_type":"stream"},{"name":"stdout","text":"FAISS index loaded successfully. Number of vectors in the index: 764\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# ---------------------------\n# 2. Query Handling & Similarity Search\n# ---------------------------\n# 2.1. Get a single text query from the user\nquery = input(\"Enter your query: \")\n\n# 2.2. Embed the query and perform a unified similarity search across all stored documents\nretrieved_docs = vector_store.similarity_search(query, k=5)\n\n# Optional: Print the retrieved document snippets for debugging purposes\nprint(\"\\nRetrieved Documents:\")\nfor i, doc in enumerate(retrieved_docs):\n    print(f\"Document {i+1} snippet:\\n{doc.page_content[:300]}\\n{'-'*80}\")\n\n# Combine the retrieved document chunks to form a context string\ncontext = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T12:52:22.392154Z","iopub.execute_input":"2025-02-15T12:52:22.392789Z","iopub.status.idle":"2025-02-15T12:52:38.886933Z","shell.execute_reply.started":"2025-02-15T12:52:22.392761Z","shell.execute_reply":"2025-02-15T12:52:38.886008Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your query:  list down the teacher teaching microeconomics\n"},{"name":"stdout","text":"\nRetrieved Documents:\nDocument 1 snippet:\nMICROECONOMICS SYED AHMED 14:30 TUE - THU 57 97067\n--------------------------------------------------------------------------------\nDocument 2 snippet:\nMICROECONOMICS Dr. Khadija Malik Bari 10:00 TUE - THU 40 96761\n--------------------------------------------------------------------------------\nDocument 3 snippet:\nMICROECONOMICS Dr. Khadija Malik Bari 11:30 TUE - THU 40 96794\n--------------------------------------------------------------------------------\nDocument 4 snippet:\nMICROECONOMICS Dr. Asad Bilal Rizvi 10:00 TUE - THU 56 96772\n--------------------------------------------------------------------------------\nDocument 5 snippet:\nPRINCIPLES OF MICROECONOMICS Sahar Arshad Mahmood 10:00 TUE - THU 55 96774\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoModelForCausalLM, AutoTokenizer, pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T12:52:43.054140Z","iopub.execute_input":"2025-02-15T12:52:43.054477Z","iopub.status.idle":"2025-02-15T12:52:43.082348Z","shell.execute_reply.started":"2025-02-15T12:52:43.054449Z","shell.execute_reply":"2025-02-15T12:52:43.081503Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T12:52:43.324777Z","iopub.execute_input":"2025-02-15T12:52:43.325026Z","iopub.status.idle":"2025-02-15T12:52:43.328364Z","shell.execute_reply.started":"2025-02-15T12:52:43.325006Z","shell.execute_reply":"2025-02-15T12:52:43.327654Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}